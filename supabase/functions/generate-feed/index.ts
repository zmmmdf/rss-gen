import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type, x-supabase-client-platform, x-supabase-client-platform-version, x-supabase-client-runtime, x-supabase-client-runtime-version',
};

interface FeedItem {
  title: string;
  description: string;
  date: string;
  link: string;
  image: string;
  content?: string;
}

function escapeXml(str: string): string {
  return str.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;').replace(/"/g, '&quot;');
}

function toXml(feedName: string, feedUrl: string, items: FeedItem[]): string {
  const itemsXml = items.map(item => `    <item>
      <title>${escapeXml(item.title)}</title>
      <link>${escapeXml(item.link)}</link>
      <description>${escapeXml(item.description)}</description>
      ${item.date ? `<pubDate>${item.date}</pubDate>` : ''}
      ${item.image ? `<enclosure url="${escapeXml(item.image)}" type="image/jpeg"/>` : ''}
      ${item.content ? `<content:encoded><![CDATA[${item.content}]]></content:encoded>` : ''}
    </item>`).join('\n');

  return `<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>${escapeXml(feedName)}</title>
    <link>${escapeXml(feedUrl)}</link>
    <description>RSS feed generated by rss-gen</description>
${itemsXml}
  </channel>
</rss>`;
}

function toJsonFeed(feedName: string, feedUrl: string, items: FeedItem[]): object {
  return {
    version: 'https://jsonfeed.org/version/1.1',
    title: feedName,
    home_page_url: feedUrl,
    items: items.map(item => ({
      id: item.link || item.title,
      title: item.title,
      url: item.link,
      summary: item.description,
      date_published: item.date || undefined,
      image: item.image || undefined,
      content_html: item.content || undefined,
    })),
  };
}

function toCsv(items: FeedItem[]): string {
  const headers = ['title', 'description', 'date', 'link', 'image', 'content'];
  const escape = (s: string) => `"${(s || '').replace(/"/g, '""')}"`;
  const rows = items.map(item => headers.map(h => escape((item as any)[h] || '')).join(','));
  return [headers.join(','), ...rows].join('\n');
}

Deno.serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    let feedId: string | null = null;
    let format = 'xml';

    // Support both GET query params and POST body
    if (req.method === 'GET') {
      const url = new URL(req.url);
      feedId = url.searchParams.get('id');
      format = url.searchParams.get('format') || 'xml';
    } else {
      const body = await req.json();
      feedId = body.id;
      format = body.format || 'xml';
    }

    if (!feedId) {
      return new Response(
        JSON.stringify({ error: 'Feed ID is required' }),
        { status: 400, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
    const supabaseKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
    const supabase = createClient(supabaseUrl, supabaseKey);

    const { data: feed, error: feedError } = await supabase
      .from('feeds')
      .select('*')
      .eq('id', feedId)
      .single();

    if (feedError || !feed) {
      return new Response(
        JSON.stringify({ error: 'Feed not found' }),
        { status: 404, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    // Scrape the source page
    const apiKey = Deno.env.get('FIRECRAWL_API_KEY');
    if (!apiKey) {
      return new Response(
        JSON.stringify({ error: 'Firecrawl not configured' }),
        { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    const scrapeRes = await fetch('https://api.firecrawl.dev/v1/scrape', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        url: feed.source_url,
        formats: ['html'],
        onlyMainContent: false,
      }),
    });

    const scrapeData = await scrapeRes.json();
    const html = scrapeData?.data?.html || scrapeData?.html || '';

    if (!html) {
      return new Response(
        JSON.stringify({ error: 'Failed to scrape source page' }),
        { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    // Parse HTML and extract items using DOMParser (available in Deno)
    // Since Deno doesn't have DOMParser natively, we'll use regex-based extraction
    const selectors = feed.list_selectors as any;

    // For server-side HTML parsing, use a simple approach
    // We'll use linkedom for parsing
    const { parseHTML } = await import('https://esm.sh/linkedom@0.16.8');
    const { document: doc } = parseHTML(html);

    const items: FeedItem[] = [];

    if (selectors?.container) {
      const containers = doc.querySelectorAll(selectors.container);
      for (let i = 0; i < containers.length && i < 50; i++) {
        const container = containers[i];
        const item: FeedItem = {
          title: '',
          description: '',
          date: '',
          link: '',
          image: '',
        };

        if (selectors.title) {
          const el = container.querySelector(selectors.title);
          if (el) item.title = el.textContent?.trim() || '';
        }
        if (selectors.description) {
          const el = container.querySelector(selectors.description);
          if (el) item.description = el.textContent?.trim() || '';
        }
        if (selectors.date) {
          const el = container.querySelector(selectors.date);
          if (el) item.date = el.textContent?.trim() || '';
        }
        if (selectors.link) {
          const el = container.querySelector(selectors.link);
          if (el) {
            let href = el.getAttribute('href') || '';
            // If element itself doesn't have href, check if it IS an anchor or walk up to find one
            if (!href) {
              const anchor = el.tagName?.toUpperCase() === 'A' ? el : el.closest?.('a') || el.querySelector?.('a');
              if (anchor) {
                href = anchor.getAttribute('href') || '';
              }
            }
            if (href && !href.startsWith('http')) {
              try {
                href = new URL(href, feed.source_url).href;
              } catch { }
            }
            item.link = href;
          }
        } else {
          // If no link selector, check if container itself is an anchor tag
          if (container.tagName === 'A' || container.tagName === 'a') {
            let href = container.getAttribute('href') || '';
            if (href && !href.startsWith('http')) {
              try {
                href = new URL(href, feed.source_url).href;
              } catch { }
            }
            item.link = href;
          } else {
            // Otherwise, get href attribute of the post container directly
            let href = container.getAttribute('href') || '';
            if (href && !href.startsWith('http')) {
              try {
                href = new URL(href, feed.source_url).href;
              } catch { }
            }
            item.link = href;

            // As a last resort, find the first <a> inside the container if still no link
            if (!item.link) {
              let href = '';
              const firstAnchor = container.querySelector('a[href]');
              if (firstAnchor) {
                href = firstAnchor.getAttribute('href') || '';
              }
              if (href && !href.startsWith('http')) {
                try {
                  href = new URL(href, feed.source_url).href;
                } catch { }
              }
              item.link = href;
            }
          }
        }
        if (selectors.image) {
          const el = container.querySelector(selectors.image);
          if (el) {
            let src = el.getAttribute('src') || el.getAttribute('data-src') || '';
            if (src && !src.startsWith('http')) {
              try {
                src = new URL(src, feed.source_url).href;
              } catch { }
            }
            item.image = src;
          }
        }

        if (item.title || item.link) {
          items.push(item);
        }
      }
    }

    // Update feed stats
    await supabase.from('feeds').update({
      item_count: items.length,
      last_scraped_at: new Date().toISOString(),
    }).eq('id', feedId);

    // Return formatted output
    if (format === 'json') {
      const jsonFeed = toJsonFeed(feed.name, feed.source_url, items);
      return new Response(
        JSON.stringify(jsonFeed, null, 2),
        { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    } else if (format === 'csv') {
      const csv = toCsv(items);
      return new Response(csv, {
        headers: { ...corsHeaders, 'Content-Type': 'text/csv', 'Content-Disposition': `attachment; filename="${feed.name}.csv"` },
      });
    } else {
      const xml = toXml(feed.name, feed.source_url, items);
      return new Response(xml, {
        headers: { ...corsHeaders, 'Content-Type': 'application/rss+xml' },
      });
    }
  } catch (error) {
    console.error('Error generating feed:', error);
    const errorMessage = error instanceof Error ? error.message : 'Failed to generate feed';
    return new Response(
      JSON.stringify({ error: errorMessage }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
  }
});
