import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type, x-supabase-client-platform, x-supabase-client-platform-version, x-supabase-client-runtime, x-supabase-client-runtime-version',
};

interface FeedItem {
  title: string;
  description: string;
  date: string;
  link: string;
  image: string;
  content?: string;
}

function escapeXml(str: string): string {
  return str.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;').replace(/"/g, '&quot;');
}

function toXml(feedName: string, feedUrl: string, items: FeedItem[]): string {
  const itemsXml = items.map(item => `    <item>
      <title>${escapeXml(item.title)}</title>
      <link>${escapeXml(item.link)}</link>
      <description>${escapeXml(item.description)}</description>
      ${item.date ? `<pubDate>${item.date}</pubDate>` : ''}
      ${item.image ? `<enclosure url="${escapeXml(item.image)}" type="image/jpeg"/>` : ''}
      ${item.content ? `<content:encoded><![CDATA[${item.content}]]></content:encoded>` : ''}
    </item>`).join('\n');

  return `<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>${escapeXml(feedName)}</title>
    <link>${escapeXml(feedUrl)}</link>
    <description>RSS feed generated by rss-gen</description>
${itemsXml}
  </channel>
</rss>`;
}

function toJsonFeed(feedName: string, feedUrl: string, items: FeedItem[]): object {
  return {
    version: 'https://jsonfeed.org/version/1.1',
    title: feedName,
    home_page_url: feedUrl,
    items: items.map(item => ({
      id: item.link || item.title,
      title: item.title,
      url: item.link,
      summary: item.description,
      date_published: item.date || undefined,
      image: item.image || undefined,
      content_html: item.content || undefined,
    })),
  };
}

function toCsv(items: FeedItem[]): string {
  const headers = ['title', 'description', 'date', 'link', 'image', 'content'];
  const escape = (s: string) => `"${(s || '').replace(/"/g, '""')}"`;
  const rows = items.map(item => headers.map(h => escape((item as any)[h] || '')).join(','));
  return [headers.join(','), ...rows].join('\n');
}

Deno.serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    let feedId: string | null = null;
    let format = 'xml';
    let previewConfig: any = null;

    // Support both GET query params and POST body
    if (req.method === 'GET') {
      const url = new URL(req.url);
      feedId = url.searchParams.get('id');
      format = url.searchParams.get('format') || 'xml';
    } else {
      const body = await req.json();
      feedId = body.id;
      format = body.format || 'xml';
      previewConfig = body.preview;
    }

    if (!feedId && !previewConfig) {
      return new Response(
        JSON.stringify({ error: 'Feed ID or preview config is required' }),
        { status: 400, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
    const supabaseKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
    const supabase = createClient(supabaseUrl, supabaseKey);

    let feed: any = null;

    if (previewConfig) {
      feed = previewConfig;
      // Provide a proxy name if none
      if (!feed.name) feed.name = 'Live Preview';
    } else {
      const { data: feedData, error: feedError } = await supabase
        .from('feeds')
        .select('*')
        .eq('id', feedId)
        .single();

      if (feedError || !feedData) {
        return new Response(
          JSON.stringify({ error: 'Feed not found' }),
          { status: 404, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        );
      }
      feed = feedData;
    }

    // Scrape the source page
    const apiKey = Deno.env.get('FIRECRAWL_API_KEY');
    if (!apiKey) {
      return new Response(
        JSON.stringify({ error: 'Firecrawl not configured' }),
        { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    const scrapeRes = await fetch('https://api.firecrawl.dev/v1/scrape', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        url: feed.source_url,
        formats: ['html'],
        onlyMainContent: false,
      }),
    });

    const scrapeData = await scrapeRes.json();
    const html = scrapeData?.data?.html || scrapeData?.html || '';

    if (!html) {
      return new Response(
        JSON.stringify({ error: 'Failed to scrape source page' }),
        { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    // Parse HTML and extract items using DOMParser (available in Deno)
    // Since Deno doesn't have DOMParser natively, we'll use regex-based extraction
    const selectors = feed.list_selectors as any;

    // For server-side HTML parsing, use a simple approach
    // We'll use linkedom for parsing
    const { parseHTML } = await import('https://esm.sh/linkedom@0.16.8');
    const { document: doc } = parseHTML(html);

    const items: FeedItem[] = [];

    if (selectors?.container) {
      const containers = doc.querySelectorAll(selectors.container);
      for (let i = 0; i < containers.length && i < 50; i++) {
        const container = containers[i];
        const item: FeedItem = {
          title: '',
          description: '',
          date: '',
          link: '',
          image: '',
        };

        if (selectors.title) {
          const el = container.querySelector(selectors.title);
          if (el) item.title = el.textContent?.trim() || '';
        }
        if (selectors.description) {
          const el = container.querySelector(selectors.description);
          if (el) item.description = el.textContent?.trim() || '';
        }
        if (selectors.date) {
          const el = container.querySelector(selectors.date);
          if (el) item.date = el.textContent?.trim() || '';
        }
        let linkEl: any = null;
        if (selectors.link) {
          if (typeof container.matches === 'function' && container.matches(selectors.link)) {
            linkEl = container;
          } else {
            linkEl = container.querySelector(selectors.link);
          }
        }

        if (linkEl) {
          let href = linkEl.getAttribute('href');
          if (!href) {
            const aTag = (linkEl.tagName || '').toUpperCase() === 'A' ? linkEl : linkEl.querySelector('a');
            if (aTag) href = aTag.getAttribute('href');
          }
          href = href || '';

          if (href && !href.startsWith('http')) {
            try {
              href = new URL(href, feed.source_url).href;
            } catch { } // fallback to relative
          }
          item.link = href;
        } else {
          // If no link selector, find the first anchor tag inside the container
          const aTag = (container.tagName || '').toUpperCase() === 'A' ? container : container.querySelector('a');
          if (aTag) {
            let href = aTag.getAttribute('href') || '';
            if (href && !href.startsWith('http')) {
              try {
                href = new URL(href, feed.source_url).href;
              } catch { }
            }
            item.link = href;
          } else {
            // Otherwise, get href attribute of the post container directly
            let href = container.getAttribute('href') || '';
            if (href && !href.startsWith('http')) {
              try {
                href = new URL(href, feed.source_url).href;
              } catch { }
            }
            item.link = href;

            // As a last resort, find the first <a> inside the container if still no link
            if (!item.link) {
              let href = '';
              const firstAnchor = container.querySelector('a[href]');
              if (firstAnchor) {
                href = firstAnchor.getAttribute('href') || '';
              }
              if (href && !href.startsWith('http')) {
                try {
                  href = new URL(href, feed.source_url).href;
                } catch { }
              }
              item.link = href;
            }
          }
        }

        if (selectors.image) {
          const el = container.querySelector(selectors.image);
          if (el) {
            let src = el.getAttribute('src') || el.getAttribute('data-src') || '';
            if (src && !src.startsWith('http')) {
              try {
                src = new URL(src, feed.source_url).href;
              } catch { }
            }
            item.image = src;
          }
        }

        if (item.title || item.link) {
          items.push(item);
        }
      }
    }

    // If we have a content_selector, fetch the content for each item
    if (feed.content_selector && items.length > 0) {
      // Limit to top 10 items to avoid edge function timeouts and rate limits
      const itemsToFetch = items.slice(0, 10);

      await Promise.all(itemsToFetch.map(async (item) => {
        if (!item.link) return;

        try {
          const contentRes = await fetch('https://api.firecrawl.dev/v1/scrape', {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${apiKey}`,
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              url: item.link,
              formats: ['html'],
              onlyMainContent: false,
            }),
          });

          if (contentRes.ok) {
            const data = await contentRes.json();
            const itemHtml = data?.data?.html || data?.html || '';

            if (itemHtml) {
              const { document: itemDoc } = parseHTML(itemHtml);
              const contentEl = itemDoc.querySelector(feed.content_selector);

              if (contentEl) {
                // Respect content_format (text vs html)
                if (feed.content_format === 'html') {
                  item.content = contentEl.innerHTML;
                } else {
                  item.content = contentEl.textContent?.trim() || '';
                }
              }
            }
          }
        } catch (err) {
          console.error(`Failed to fetch content for ${item.link}:`, err);
        }
      }));
    }

    // Update feed stats
    if (feedId) {
      await supabase.from('feeds').update({
        item_count: items.length,
        last_scraped_at: new Date().toISOString(),
      }).eq('id', feedId);
    }

    // Return formatted output
    if (format === 'json') {
      const jsonFeed = toJsonFeed(feed.name, feed.source_url, items);
      return new Response(
        JSON.stringify(jsonFeed, null, 2),
        { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    } else if (format === 'csv') {
      const csv = toCsv(items);
      return new Response(csv, {
        headers: { ...corsHeaders, 'Content-Type': 'text/csv', 'Content-Disposition': `attachment; filename="${feed.name}.csv"` },
      });
    } else {
      const xml = toXml(feed.name, feed.source_url, items);
      return new Response(xml, {
        headers: { ...corsHeaders, 'Content-Type': 'application/rss+xml' },
      });
    }
  } catch (error) {
    console.error('Error generating feed:', error);
    const errorMessage = error instanceof Error ? error.message : 'Failed to generate feed';
    return new Response(
      JSON.stringify({ error: errorMessage }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
  }
});
